{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import contractions\n",
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from simpletransformers.classification import (ClassificationArgs,\n",
    "                                               ClassificationModel)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import logging\n",
    "\n",
    "from dont_patronize_me import DontPatronizeMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logging.set_verbosity_error()\n",
    "sns.set_theme()\n",
    "tqdm.pandas()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'coursework.ipynb'\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "GLOBAL_SEED = 0\n",
    "\n",
    "def seed_everything(seed=GLOBAL_SEED):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(f\"Global seed set to {seed}\")\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/dontpatronizeme_pcl.tsv\",\n",
    "    sep='\\t',\n",
    "    names=['par_id', 'art_id', 'community', 'country', 'text', 'labels'], skiprows=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "df['labels'] = df[\"labels\"].progress_apply(lambda x: \"No PCL\" if x in [0, 1] else \"PCL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "df[\"labels\"].value_counts(sort=False).plot(kind=\"bar\")\n",
    "\n",
    "plt.xlabel(None)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/pcl_labels.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "keyword_df = df.filter(items=['labels', 'community'])\n",
    "keyword_df[\"community\"] = keyword_df[\"community\"].str.title()\n",
    "keyword_df = keyword_df.groupby(['community', 'labels']).size().unstack(fill_value=0)\n",
    "keyword_df.plot(ax=ax, kind='barh', stacked=True, legend=True)\n",
    "ax.legend([\"No PCL\", \"PCL\"])\n",
    "\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    ax.annotate(f'{width:.0f}', (p.get_x() + width / 2, p.get_y()-0.13), ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=10, color='white')\n",
    "\n",
    "plt.ylabel('Community')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/community.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "country_df = df.filter(items=['labels', 'country'])\n",
    "country_df[\"country\"] = country_df[\"country\"].str.upper()\n",
    "country_df = country_df.groupby(['country', 'labels']).size().unstack(fill_value=0)\n",
    "country_df.plot(ax=ax, kind='barh', stacked=True, legend=True)\n",
    "ax.legend([\"No PCL\", \"PCL\"], loc='upper left')\n",
    "\n",
    "plt.ylabel('Country')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/country.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "length_df = df.copy(deep=True)\n",
    "length_df[\"text\"] = length_df[\"text\"].astype(str).progress_apply(len)\n",
    "length_df = length_df.filter(items=[\"labels\", \"text\"])\n",
    "\n",
    "hist = length_df.plot(\n",
    "    kind=\"hist\",\n",
    "    by=\"labels\",\n",
    "    bins=20,\n",
    "    range=(0, 1500),\n",
    "    subplots=True,\n",
    "    sharex=True,\n",
    "    xlabel= \"Text length\",\n",
    "    ylabel= \"Frequency\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/text_length.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = df.copy(deep=True)\n",
    "length_df[\"text\"] = length_df[\"text\"].astype(str).progress_apply(len)\n",
    "length_df = length_df[[\"text\", \"labels\"]]\n",
    "print(\"No PCL median: \", length_df[length_df[\"labels\"] == 0][\"text\"].median())\n",
    "print(\"PCL median: \", length_df[length_df[\"labels\"] == 1][\"text\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def load_and_split_data():\n",
    "    dpm = DontPatronizeMe(\"data\", \"data\")\n",
    "\n",
    "    dpm.load_task1()\n",
    "\n",
    "    trids = pd.read_csv(\"data/train_semeval_parids-labels.csv\")\n",
    "    teids = pd.read_csv(\"data/dev_semeval_parids-labels.csv\")\n",
    "\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    data = dpm.train_task1_df\n",
    "\n",
    "    train = []\n",
    "    for idx in range(len(trids)):\n",
    "        parid = trids.par_id[idx]\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        country = data.loc[data.par_id == parid].country.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        labels = data.loc[data.par_id == parid].labels.values[0]\n",
    "        orig_label = data.loc[data.par_id == parid].orig_label.values[0]\n",
    "        train.append(\n",
    "            {\n",
    "                \"community\": keyword,\n",
    "                \"country\": country,\n",
    "                \"text\": text,\n",
    "                \"labels\": labels,\n",
    "                \"orig_label\": orig_label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    test = []\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        country = data.loc[data.par_id == parid].country.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        labels = data.loc[data.par_id == parid].labels.values[0]\n",
    "        orig_label = data.loc[data.par_id == parid].orig_label.values[0]\n",
    "        test.append(\n",
    "            {\n",
    "                \"community\": keyword,\n",
    "                \"country\": country,\n",
    "                \"text\": text,\n",
    "                \"labels\": labels,\n",
    "                \"orig_label\": orig_label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(train), pd.DataFrame(test)\n",
    "\n",
    "def labels2file(p, outf_path):\n",
    "\twith open(outf_path,'w') as outf:\n",
    "\t\tfor pi in p:\n",
    "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def augment_and_rebalance(data, aug, filename=None):\n",
    "    data = data.dropna()\n",
    "    data_0 = data[data[\"labels\"] == 0]\n",
    "    data_1 = data[data[\"labels\"] == 1]\n",
    "    n = len(data_0) // len(data_1)\n",
    "    augmented = [data]\n",
    "    print(f\"Augmenting {n} times\")\n",
    "    for i in range(n):\n",
    "        data_1_copy = data_1.copy(deep=True)\n",
    "        data_1_copy[\"text\"] = data_1_copy[\"text\"].progress_apply(lambda x: aug.augment(x)[0])\n",
    "        augmented.append(data_1_copy)\n",
    "        print(f\"Augmentation {i+1} complete\")\n",
    "    print(f\"All augmentations complete\")\n",
    "    final = pd.concat(augmented, ignore_index=True, axis=0)\n",
    "    if filename:\n",
    "        final.to_csv(f\"data/{filename}.csv\", index=False)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def preprocessor(text, processes):\n",
    "    text = re.sub(\"<h>\", \"\", text) # Remove <h> tags\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text) # Replace n't with not\n",
    "    text = re.sub(r\"\\\"\", \"\", text) # Remove quotation marks\n",
    "    text = text.replace(\" '\", \"\") # Remove spaces before apostrophes\n",
    "    text = contractions.fix(text) # Expand contractions\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # Remove extra whitespace\n",
    "\n",
    "    if \"lowercase\" in processes:\n",
    "        text = text.lower()\n",
    "\n",
    "    if \"punctuation\" in processes:\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if \"stopwords\" in processes:\n",
    "        tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    if \"tokenize\" in processes:\n",
    "        return tokens\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess(data, processes, filename=None):\n",
    "    data = data.dropna()\n",
    "    data[\"text\"] = data[\"text\"].progress_apply(lambda x: preprocessor(x, processes))\n",
    "    if filename:\n",
    "        data.to_csv(f\"data/{filename}.csv\", index=False)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def save_augmentations():\n",
    "    train, _ = load_and_split_data()\n",
    "    train, val = train_test_split(\n",
    "        train, test_size=0.2, shuffle=True, random_state=GLOBAL_SEED\n",
    "    )\n",
    "\n",
    "    train.to_csv(f\"data/train_0.8.csv\", index=False)\n",
    "    val.to_csv(f\"data/val_0.2.csv\", index=False)\n",
    "\n",
    "    for aug in [\n",
    "        nac.KeyboardAug(),\n",
    "        nac.RandomCharAug(action=\"swap\"),\n",
    "        naw.SynonymAug(),\n",
    "        naw.AntonymAug(),\n",
    "        naw.ContextualWordEmbsAug(),\n",
    "        nas.ContextualWordEmbsForSentenceAug(),\n",
    "    ]:\n",
    "        augment_and_rebalance(\n",
    "            train.copy(deep=True),\n",
    "            aug=aug,\n",
    "            filename=f\"train_0.8_{aug.name.lower()}\",\n",
    "        )\n",
    "\n",
    "# save_augmentations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Fine Tuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def fine_tune(\n",
    "    train,\n",
    "    val,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    schedule,\n",
    "):\n",
    "    args = ClassificationArgs(\n",
    "        output_dir=\"outputs\",\n",
    "        train_batch_size=batch_size,\n",
    "        eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        scheduler=schedule,\n",
    "        use_early_stopping=True,\n",
    "        overwrite_output_dir=True,\n",
    "        use_multiprocessing=False,\n",
    "        use_multiprocessing_for_evaluation=False,\n",
    "    )\n",
    "    model = ClassificationModel(\n",
    "        \"roberta\",\n",
    "        \"roberta-base\",\n",
    "        args=args,\n",
    "        use_cuda=torch.cuda.is_available(),\n",
    "        num_labels=2,\n",
    "    )\n",
    "    model.train_model(train)\n",
    "    results, outputs, wrong = model.eval_model(val)\n",
    "    print(classification_report(val[\"labels\"], np.argmax(outputs, axis=-1)))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def augmentation_flow():\n",
    "    val = pd.read_csv(\"data/val_0.2.csv\")\n",
    "    val = preprocess(val, processes={})\n",
    "    val = val[[\"text\", \"labels\"]]\n",
    "\n",
    "    train = pd.read_csv(\"data/train_0.8.csv\")\n",
    "    train = preprocess(train, processes={})\n",
    "    train = train[[\"text\", \"labels\"]]\n",
    "\n",
    "    for train_aug_file in [\n",
    "        \"train_0.8_keyboard_aug\",\n",
    "        \"train_0.8_randomchar_aug\",\n",
    "        \"train_0.8_synonym_aug\",\n",
    "        \"train_0.8_antonym_aug\",\n",
    "        \"train_0.8_contextualwordembs_aug\",\n",
    "        \"train_0.8_contextualwordembsforsentence_aug\",\n",
    "    ]:\n",
    "        train_aug = pd.read_csv(f\"data/{train_aug_file}.csv\")\n",
    "        train_aug = preprocess(train_aug, processes={})\n",
    "        train_aug = train_aug[[\"text\", \"labels\"]]\n",
    "\n",
    "        print(f\"Augmentation: {train_aug_file}\")\n",
    "\n",
    "        model = fine_tune(\n",
    "            train_aug,\n",
    "            val,\n",
    "            epochs=5,\n",
    "            batch_size=128,\n",
    "            lr=1e-5,\n",
    "            schedule=\"linear_schedule_with_warmup\",\n",
    "        )\n",
    "        predictions, _ = model.predict(train[\"text\"].values.tolist())\n",
    "        print(classification_report(train[\"labels\"], predictions))\n",
    "\n",
    "\n",
    "# augmentation_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def preprocessing_flow():\n",
    "    for processes in [\n",
    "        {},\n",
    "        {\"lowercase\"},\n",
    "        {\"punctuation\"},\n",
    "        {\"stopwords\"},\n",
    "        {\"stopwords\", \"punctuation\"},\n",
    "        {\"stopwords\", \"lowercase\"},\n",
    "        {\"punctuation\", \"lowercase\"},\n",
    "        {\"stopwords\", \"punctuation\", \"lowercase\"},\n",
    "    ]:\n",
    "        train = pd.read_csv(\"data/train_0.8.csv\")\n",
    "        train = preprocess(train, processes=processes)\n",
    "        train = train[[\"text\", \"labels\"]]\n",
    "\n",
    "        val = pd.read_csv(\"data/val_0.2.csv\")\n",
    "        val = preprocess(val, processes=processes)\n",
    "        val = val[[\"text\", \"labels\"]]\n",
    "\n",
    "        ps = \", \".join(processes)\n",
    "        print(f\"Preprocessing: {ps}\")\n",
    "\n",
    "        model = fine_tune(\n",
    "            train,\n",
    "            val,\n",
    "            epochs=5,\n",
    "            batch_size=128,\n",
    "            lr=1e-5,\n",
    "            schedule=\"linear_schedule_with_warmup\",\n",
    "        )\n",
    "        predictions, _ = model.predict(train[\"text\"].values.tolist())\n",
    "        print(classification_report(train[\"labels\"], predictions))\n",
    "\n",
    "\n",
    "# preprocessing_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def get_embeddings(data):\n",
    "    # word2vec_model = gensim.downloader.load(\"glove-twitter-200\")\n",
    "    # vec_size = 200\n",
    "\n",
    "    word2vec_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    vec_size = 300\n",
    "\n",
    "    def word2vec(text, model):\n",
    "        vecs = []\n",
    "        for word in text:\n",
    "            if word in model:\n",
    "                vecs.append(model[word])\n",
    "            else:\n",
    "                vecs.append(np.zeros(vec_size))\n",
    "        return np.mean(vecs, axis=0)\n",
    "\n",
    "    for x in [\"country\", \"community\"]:\n",
    "        data[x] = pd.Categorical(data[x], categories=data[x].unique()).codes\n",
    "\n",
    "    data[\"length\"] = data[\"text\"].progress_apply(len)\n",
    "    data[\"text\"] = data[\"text\"].progress_apply(lambda x: word2vec(x, word2vec_model))\n",
    "    data[np.arange(vec_size)] = data[\"text\"].progress_apply(lambda x: pd.Series(x))\n",
    "    data = data.drop(columns=[\"text\"])\n",
    "    data = data.dropna()\n",
    "\n",
    "    data.columns = data.columns.astype(str)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def baselines_flow():\n",
    "    train, test = load_and_split_data()\n",
    "    train_original = train.copy(deep=True)\n",
    "\n",
    "    train = preprocess(train, processes={\"tokenize\"})\n",
    "    test = preprocess(test, processes={\"tokenize\"})\n",
    "\n",
    "    train = get_embeddings(train)\n",
    "    test = get_embeddings(test)\n",
    "\n",
    "    trainY = train[[\"labels\"]]\n",
    "    testY = test[[\"labels\"]]\n",
    "\n",
    "    trainX = train.drop(columns=[\"labels\"])\n",
    "    testX = test.drop(columns=[\"labels\"])\n",
    "\n",
    "    if 'orig_label' in df:\n",
    "        trainX = trainX.drop(columns=[\"orig_label\"])\n",
    "        testX = testX.drop(columns=[\"orig_label\"])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    trainX = scaler.fit_transform(trainX)\n",
    "    testX = scaler.transform(testX)\n",
    "\n",
    "    for model in [LogisticRegression(max_iter=1000), ComplementNB()]:\n",
    "        model.fit(trainX, trainY)\n",
    "        model_name = type(model).__name__\n",
    "        print(f\"{model_name} train: \\n\", classification_report(trainY, model.predict(trainX)))\n",
    "        print(f\"{model_name} test: \\n\", classification_report(testY, model.predict(testX)))\n",
    "        print(f\"{model_name} misclassifications: \\n\", train_original[\"text\"][trainY[\"labels\"] != model.predict(trainX)])\n",
    "\n",
    "\n",
    "# baselines_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def hyperparameter_tuning_flow():\n",
    "    train = pd.read_csv(\"data/train_0.8.csv\")\n",
    "    train = preprocess(train, processes={\"lowercase\"})\n",
    "    train = train[[\"text\", \"labels\"]]\n",
    "\n",
    "    train_aug = pd.read_csv(\"data/train_0.8_keyboard_aug.csv\")\n",
    "    train_aug = preprocess(train_aug, processes={\"lowercase\"})\n",
    "    train_aug = train_aug[[\"text\", \"labels\"]]\n",
    "\n",
    "    val = pd.read_csv(\"data/val_0.2.csv\")\n",
    "    val = preprocess(val, processes={\"lowercase\"})\n",
    "    val = val[[\"text\", \"labels\"]]\n",
    "\n",
    "    epochs = 5\n",
    "    lrs = [1e-4, 1e-5]\n",
    "    batch_sizes = [128, 256]\n",
    "    schedules = [\"linear_schedule_with_warmup\", \"constant_schedule_with_warmup\"]\n",
    "\n",
    "    for schedule in schedules:\n",
    "        for lr in lrs:\n",
    "            for batch_size in batch_sizes:\n",
    "                model = fine_tune(\n",
    "                    train_aug,\n",
    "                    val,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    lr=lr,\n",
    "                    schedule=schedule,\n",
    "                )\n",
    "                predictions, _ = model.predict(train[\"text\"].values.tolist())\n",
    "                print(classification_report(train[\"labels\"], predictions))\n",
    "                print(\n",
    "                    f\"Schedule: {schedule}, learning rate: {lr}, batch size: {batch_size}\"\n",
    "                )\n",
    "\n",
    "\n",
    "# hyperparameter_tuning_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "def final_model_flow():\n",
    "    train, dev = load_and_split_data()\n",
    "\n",
    "    train = preprocess(\n",
    "        train,\n",
    "        processes={\"lowercase\"},\n",
    "    )\n",
    "    train = train[[\"text\", \"labels\"]]\n",
    "\n",
    "    train_aug = train.copy(deep=True)\n",
    "    train_aug = augment_and_rebalance(train_aug, nac.KeyboardAug())\n",
    "    train_aug = preprocess(\n",
    "        train_aug,\n",
    "        processes={\"lowercase\"},\n",
    "    )\n",
    "    train_aug = train_aug[[\"text\", \"labels\"]]\n",
    "\n",
    "    dev = preprocess(\n",
    "        dev,\n",
    "        processes={\"lowercase\"},\n",
    "    )\n",
    "    dev = dev[[\"text\", \"labels\"]]\n",
    "\n",
    "    test = pd.read_csv(\"data/task4_test.tsv\",sep='\\t', names=['par_id', 'art_id', 'community', 'country', 'text'])\n",
    "    test = preprocess(test, processes={\"lowercase\"})\n",
    "\n",
    "    optimal = {\"lr\": 1e-4, \"batch_size\": 256, \"schedule\": \"linear_schedule_with_warmup\"}\n",
    "    epochs = 10\n",
    "\n",
    "    model = fine_tune(\n",
    "        train_aug,\n",
    "        dev,\n",
    "        epochs=epochs,\n",
    "        batch_size=optimal[\"batch_size\"],\n",
    "        lr=optimal[\"lr\"],\n",
    "        schedule=optimal[\"schedule\"],\n",
    "    )\n",
    "\n",
    "    predictions, _ = model.predict(train[\"text\"].values.tolist())\n",
    "    print(classification_report(train[\"labels\"], predictions))\n",
    "\n",
    "    predictions, _ = model.predict(dev[\"text\"].values.tolist())\n",
    "    labels2file([[p] for p in predictions], 'dev.txt')\n",
    "\n",
    "    predictions, _ = model.predict(test[\"text\"].values.tolist())\n",
    "    labels2file([[p] for p in predictions], 'test.txt')\n",
    "\n",
    "    return model, dev\n",
    "\n",
    "\n",
    "model, dev = final_model_flow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dev = load_and_split_data()\n",
    "dev = preprocess(\n",
    "    dev,\n",
    "    processes={\"lowercase\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To what extent is the model better at predicting examples with a higher level of patronizing content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patronization_level_analysis(model, dev):\n",
    "    levels = [\"2\", \"3\", \"4\"]\n",
    "    f1_scores = []\n",
    "    for level in levels:\n",
    "        dev_level = dev[dev[\"orig_label\"] == level]\n",
    "        predictions, _ = model.predict(dev_level[\"text\"].values.tolist())\n",
    "        print(f\"Level: {level}\")\n",
    "        f1 = f1_score(dev_level[\"labels\"], predictions)\n",
    "        print(f1)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    plt.bar(levels, f1_scores)\n",
    "    plt.xlabel(\"PCL level\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graphs/pcl_level_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "patronization_level_analysis(model, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the length of the input sequence impact the model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_len_analysis(model, dev):\n",
    "    seq_lens = [64, 128, 256, 512]\n",
    "    f1_scores = []\n",
    "    for seq_len in seq_lens:\n",
    "        dev_len = dev.copy(deep=True)\n",
    "        dev_len[\"text\"] = dev_len[\"text\"].progress_apply(lambda x: x[:seq_len])\n",
    "        predictions, _ = model.predict(dev_len[\"text\"].values.tolist())\n",
    "        print(f\"Max sequence length: {seq_len}\")\n",
    "        f1 = f1_score(dev_len[\"labels\"], predictions)\n",
    "        print(f1)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    plt.bar([str(s) for s in seq_lens], f1_scores)\n",
    "    plt.xlabel(\"Maximum sequence length\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graphs/seq_lens_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "seq_len_analysis(model, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To what extent does model performance depend on the data categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_anaysis(model, dev):\n",
    "    communities = dev[\"community\"].unique()\n",
    "    f1_scores = []\n",
    "    for community in communities:\n",
    "        dev_community = dev[dev[\"community\"] == community]\n",
    "        predictions, _ = model.predict(dev_community[\"text\"].values.tolist())\n",
    "        print(f\"Community: {community}\")\n",
    "        f1 = f1_score(dev_community[\"labels\"], predictions)\n",
    "        print(f1)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    plt.bar([s.title() for s in communities], f1_scores)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel(\"Community\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.savefig(\"graphs/community_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "category_anaysis(model, dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
